{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook introduces object removal from the input images. The implementation incorporates mask selection using the SAM model and applies diffusion by making use of Latent Diffusion Model. In this notebook, we will provide a step-by-step guide to erasing the unwanted elements from a given image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "input_path = '../inputs/eraser/oscar3.png'\n",
    "input_image = Image.open(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a mask, using previously introduced SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask_sam import get_mask\n",
    "\n",
    "mask_image = get_mask(input_image, ([[200, 400]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(input_image)\n",
    "axs[0].axis('off')  # Hide the axis\n",
    "axs[0].set_title('Original image')  # Optionally set a title\n",
    "\n",
    "# Display the second image in the right subplot\n",
    "axs[1].imshow(mask_image)\n",
    "axs[1].axis('off')  # Hide the axis\n",
    "axs[1].set_title('Mask image')  # Optionally set a title\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the number of inference steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the original image to $512$ x $512$, which is a prerequisite image size for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = input_image\n",
    "original_width, original_height = image.size\n",
    "\n",
    "if original_width > original_height:\n",
    "    image = image.resize((768, 512))\n",
    "elif original_width < original_height:\n",
    "    image = image.resize((512, 768))\n",
    "else:\n",
    "    image = image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the mask image to $512$ x $512$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_image\n",
    "\n",
    "mask_original_width, mask_original_height = mask.size\n",
    "\n",
    "if mask_original_width > mask_original_height:\n",
    "    mask = mask.resize((768, 512))\n",
    "elif mask_original_width < mask_original_height:\n",
    "    mask = mask.resize((512, 768))\n",
    "else:\n",
    "    mask = mask.resize((512, 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the configuration and model paths of Latent Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "steps = int(ddim_steps)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(current_dir), '..'))\n",
    "\n",
    "sys.path.append(os.path.join(parent_dir, 'latent-diffusion'))\n",
    "sys.path.append(os.path.join(parent_dir, 'code'))\n",
    "\n",
    "from main import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "config = OmegaConf.load(\"../models/ldm_inpainting/config.yaml\")\n",
    "model = instantiate_from_config(config.model)\n",
    "model.load_state_dict(torch.load(\"../models/ldm_inpainting/last.ckpt\")[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer the model on the input image and mask to obtain the output image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eraser_ldm import make_batch_gradio\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "\n",
    "        batch = make_batch_gradio(image, mask, device=device)\n",
    "\n",
    "        c = model.cond_stage_model.encode(batch[\"masked_image\"])\n",
    "        cc = torch.nn.functional.interpolate(batch[\"mask\"],\n",
    "                                            size=c.shape[-2:])\n",
    "        c = torch.cat((c, cc), dim=1)\n",
    "\n",
    "        shape = (c.shape[1]-1,)+c.shape[2:]\n",
    "        samples_ddim, _ = sampler.sample(S=steps, ## changed\n",
    "                                        conditioning=c,\n",
    "                                        batch_size=c.shape[0],\n",
    "                                        shape=shape,\n",
    "                                        verbose=False)\n",
    "        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "\n",
    "        image = torch.clamp((batch[\"image\"]+1.0)/2.0,\n",
    "                            min=0.0, max=1.0)\n",
    "        mask = torch.clamp((batch[\"mask\"]+1.0)/2.0,\n",
    "                        min=0.0, max=1.0)\n",
    "        predicted_image = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
    "                                    min=0.0, max=1.0)\n",
    "\n",
    "        inpainted = (1-mask)*image+mask*predicted_image\n",
    "        inpainted = inpainted.cpu().numpy().transpose(0,2,3,1)[0]*255\n",
    "        inpainted = Image.fromarray(inpainted.astype(np.uint8))\n",
    "        inpainted = inpainted.resize((original_width, original_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inpainted)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(input_image)\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# Display the second image in the right subplot\n",
    "axs[1].imshow(inpainted)\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('Final image')\n",
    "\n",
    "# Adjust layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-editing-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
