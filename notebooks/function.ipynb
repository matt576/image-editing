{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image-Editing Functionalities Visualisation\n",
    "## Description\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive guide to various methods for image editing using deep learning models.\n",
    "It includes implementations of popular image-editing techniques such as image inpainting, super-resolution, background replacement and more.\n",
    "Each section of the notebook contains detailed explanations, code snippets, and example results to help users understand and apply these techniques.\n",
    "\n",
    "#### Checkpoints:\n",
    "###### SAM:\n",
    "Model:  facebook/sam-vit-base <br />\n",
    "Pipeline: facebook/sam-vit-base <br />\n",
    "###### ControlNet:\n",
    "Model: lllyasviel/control_v11p_sd15_inpaint <br />\n",
    "Pipeline: runwayml/stable-diffusion-v1-5 <br />\n",
    "\n",
    "Error: ControlNet generates black images for dtype.float16: <br />\n",
    "Error: Output produced contains NSFW content -> set safety_checker=None, requires_safety_checker=False for loading pipe\n",
    "\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python mask_func.py\n",
    "```\n",
    "### 2. Conditional Inpainting (Mask: Manual -> Diffusion: ControlNet Inpainting)\n",
    "#### Checkpoints:\n",
    "###### ControlNet:\n",
    "Model: lllyasviel/control_v11p_sd15_inpaint <br />\n",
    "Pipeline: runwayml/stable-diffusion-v1-5 <br />\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python inpaint_func.py\n",
    "```\n",
    "\n",
    "### 3. Magic Eraser (Mask: SAM -> Diffusion: Latent Diffusion)\n",
    "#### Checkpoints:\n",
    "###### SAM:\n",
    "Model:  facebook/sam-vit-base <br />\n",
    "Pipeline: facebook/sam-vit-base <br />\n",
    "###### Latent Diffusion:\n",
    "Model: https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1\n",
    "```bash\n",
    "wget -O models/ldm_inpainting/last.ckpt https://heibox.uni-heidelberg.de/f/4d9ac7ea40c64582b7c9/?dl=1\n",
    "```\n",
    "\n",
    "Additional imports:\n",
    "```bash\n",
    "pip install omegaconf==2.1.1\n",
    "pip install pytorch-lightning==1.6.1  # possibly newer version\n",
    "pip install einops==0.3.0\n",
    "pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "```\n",
    "In the file: **image-editing\\src\\taming-transformers\\taming\\data\\utils.py**\n",
    "\n",
    "Change line 11:\n",
    "```bash\n",
    "from torch._six import string_classes\n",
    "```\n",
    "to\n",
    "```bash\n",
    "string_classes = str\n",
    "```\n",
    "Due to a no longer existing module _six since Pytorch 1.10. <br />\n",
    "Error: ControlNet generates black images for dtype.float16: <br />\n",
    "Error: Output produced contains NSFW content -> set safety_checker=None, requires_safety_checker=False for loading pipe\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python inpaint_ldm.py --indir inputs/example_dog --outdir outputs/inpainting_results --steps 5\n",
    "```\n",
    "\n",
    "### 4. Background Extraction (Mask: Depth Anything -> Diffusion: Latent Diffusion)\n",
    "Additional imports:\n",
    "```bash\n",
    "pip install scikit-learn\n",
    "```\n",
    "The method applies the Depth-Anything model to perform depth estimation.\n",
    "In order to extract the foreground and background of the image to apply background blurring/restyling we can use the following techniques:\n",
    "1. Thresholding\n",
    "2. K-Means\n",
    "3. Fine-tune the model on foreground and background annotated using SAM\n",
    "- Stanford Background Dataset: https://www.kaggle.com/datasets/balraj98/stanford-background-dataset (715 320x240 colored images)\n",
    "- Fine-tuning necessary (!)\n",
    "#### Checkpoints:\n",
    "###### Depth Anything:\n",
    "Model: LiheYoung/depth-anything-small-hf\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python inpaint_ldm.py --indir inputs/example_dog --outdir outputs/inpainting_results --steps 5\n",
    "```\n",
    "\n",
    "### 5. GroundedSAM-based mask generation\n",
    "You should set the environment variable manually as follows if you want to build a local GPU environment for Grounded-SAM:\n",
    "\n",
    "First, to check which cuda versions are available and the required path:\n",
    "```bash\n",
    "module avail\n",
    "```\n",
    "Then:\n",
    "```bash\n",
    "source /etc/profile.d/lmod.sh  \n",
    "module load cuda/12.1.0 # Should match cuda version from pytorch\n",
    "echo $CUDA_HOME #check if variable was automatically set to /storage/software/cuda/cuda-12.1.0, otherwise set manually with EXPORT...\n",
    "```\n",
    "```bash\n",
    "export AM_I_DOCKER=False\n",
    "export BUILD_WITH_CUDA=True\n",
    "export CUDA_HOME=/storage/software/cuda/cuda-12.1.0 # Path on atcremers60@in.tum.de\n",
    "```\n",
    "\n",
    "Install Segment Anything:\n",
    "\n",
    "```bash\n",
    "python -m pip install -e ../Grounded-Segment-Anything/segment_anything\n",
    "```\n",
    "\n",
    "Install Grounding DINO:\n",
    "\n",
    "```bash\n",
    "pip install --no-build-isolation -e ../Grounded-Segment-Anything/GroundingDINO # Follow previous CUDA_HOME steps carefully\n",
    "```\n",
    "#### Checkpoints:\n",
    "```bash\n",
    "wget -O models/sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "wget -O models/groundingdino_swint_ogc.pth https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
    "```\n",
    "##### GroundingDINO:\n",
    "code/models/groundingdino_swint_ogc.pth\n",
    "\n",
    "Also, copy the GroundingDINO folder into the code folder as well as a temporary solution, since for some reason in the code folder without it, GroundingDino is not recognized as an import module inside the script.\n",
    "##### SAM:\n",
    "code/models/sam_vit_h_4b8939.pth\n",
    "\n",
    "###### Input Command Exmple:\n",
    "Specify via Text Prompt the object you want to detect and get the mask of. <br />\n",
    "Until cudatoolkit and CUDA_PATH issues get resolved, the program runs on cpu only mode, so specify it in the respective flag. If device = \"cuda\", follwing error happnes if you dont follow the CUDA_HOME variable related steps in the grounding DINO installation:\n",
    "\"NameError: name '_C' is not defined\"\n",
    "\n",
    "```bash\n",
    "python groundedsam_func.py   --config ../Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py   --grounded_checkpoint models/groundingdino_swint_ogc.pth   --sam_checkpoint models/sam_vit_h_4b8939.pth   --input_image inputs/example_dog/dog.png   --output_dir \"outputs/grounded_sam/\"   --box_threshold 0.3   --text_threshold 0.25   --text_prompt \"dog\"   --device \"cuda\"\n",
    "```\n",
    "### 6. GroundedSAM-based inpainting\n",
    "#### Checkpoints:\n",
    "same as above\n",
    "###### Input Command Exmple:\n",
    "Specify via Text Prompt the object you want to detect and the object you want to replace it with. <br />\n",
    "\n",
    "```bash\n",
    "python groundedsam_inpaint.py   --config ../Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py   --grounded_checkpoint models/groundingdino_swint_ogc.pth   --sam_checkpoint models/sam_vit_h_4b8939.pth   --input_image inputs/example_dog/dog.png   --output_dir \"outputs/grounded_sam\"   --box_threshold 0.3   --text_threshold 0.25   --det_prompt \"dog\"   --inpaint_prompt \"bear cub, high quality, detailed\"   --device \"cuda\"\n",
    " ```\n",
    "\n",
    "### 7. Using the gradio app for groundedSAM\n",
    "\n",
    "```bash\n",
    "cd Grounded-Segment-Anything\n",
    "python gradio_app.py\n",
    "```\n",
    "\n",
    "For it to run properly, the following modifications were performed:\n",
    "\n",
    "Lines 196/197: change \"image\" to \"composite\" and \"mask\" to \"layers\" <br />\n",
    "Line 372: input_image = gr.ImageEditor(sources='upload', type=\"pil\", value=\"assets/demo2.jpg\") <br />\n",
    "Line 376: run_button = gr.Button() <br />\n",
    "Lines 391-394: with gr.Column():\n",
    "                gallery = gr.Gallery(\n",
    "                label=\"Generated images\", \n",
    "                show_label=False, \n",
    "                elem_id=\"gallery\", \n",
    "                preview=True, \n",
    "                object_fit=\"scale-down\"\n",
    "                ) <br />\n",
    "Line 399: just comment out or remove <br />\n",
    "Line 400 (optional): change share=True if you need a public link <br />\n",
    "\n",
    "### 8. Background Blurring (Depth Aynthing)\n",
    "Additional imports:\n",
    "```bash\n",
    "pip install opencv-python\n",
    "```\n",
    "The method applies the Depth-Anything model to perform depth estimation.\n",
    "In order to extract the foreground and background of the image to apply background blurring/restyling we can use the following techniques:\n",
    "1. Splitting the output to several sections depending on the distance measurement\n",
    "2. Thresholding and applying varable blurring filter\n",
    "3. Merging the individual images to a single output\n",
    "\n",
    "#### Checkpoints:\n",
    "###### Depth Anything:\n",
    "Model: LiheYoung/depth_anything_vitl14\n",
    "\n",
    "Example:\n",
    "```bash\n",
    "python blurring.py\n",
    "```\n",
    "Errors: The path for directory needs to be modified. File Depth-Anything/depth_anything/dpt.py\n",
    "in lines [147, 149] should have path leading to the model.\n",
    "\n",
    "### 9. Background Replacement (RMBG-1.4)\n",
    "Additional imports:\n",
    "```bash\n",
    "pip install -qr https://huggingface.co/briaai/RMBG-1.4/resolve/main/requirements.txt\n",
    "pip install controlnet_aux\n",
    "```\n",
    "The method extracts the foreground subject from the original image.\n",
    "In the following, \n",
    "\n",
    "\n",
    "#### Checkpoints:\n",
    "###### Depth Anything:\n",
    "Model: \n",
    "\n",
    "Errors: The path for directory needs to be modified. File Depth-Anything/depth_anything/dpt.py\n",
    "in lines [147, 149] should have path leading to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
